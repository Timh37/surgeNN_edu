{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10c3b30a-a437-466e-b496-b20948ad1f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-25 14:08:29.508761: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-09-25 14:08:29.580619: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import os\n",
    "from surgeNN import io\n",
    "from surgeNN.evaluation import rmse, compute_precision, compute_recall, compute_f1, add_error_metrics_to_prediction_ds\n",
    "from surgeNN.preprocessing import deseasonalize_da\n",
    "import gcsfs\n",
    "fs = gcsfs.GCSFileSystem() #list stores, stripp zarr from filename, load "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87747106-0dbc-4d49-8758-b10f5eef80c1",
   "metadata": {},
   "source": [
    "Evaluates CoDEC simulations in the same way as the NN predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9c1ab0a-2844-44c5-9d15-db0df8f257fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#configure the script\n",
    "tgs        = ['stavanger-svg-nor-nhs.csv','wick-wic-gbr-bodc.csv','esbjerg-esb-dnk-dmi.csv','immingham-imm-gbr-bodc.csv','den_helder-denhdr-nld-rws.csv', 'fishguard-fis-gbr-bodc.csv',  'brest-822a-fra-uhslc.csv', 'vigo-vigo-esp-ieo.csv',  'alicante_i_outer_harbour-alio-esp-da_mm.csv']\n",
    "tgnames = ['Stavanger (NOR)','Wick (UK)', 'Esbjerg (DK)','Immingham (UK)','Den Helder (NL)','Fishguard (UK)','Brest (FR)','Vigo (PT)', 'Alicante (SP)']\n",
    "\n",
    "qnts = np.array([.95,.98,.99,.995]) #quantiles, don't touch\n",
    "\n",
    "max_timesteps_between_extremes = 3\n",
    "\n",
    "save_performance = 1\n",
    "\n",
    "gtsm_path = '/home/jovyan/surgeNN/input/CoDEC_ERA5_at_gesla3_tgs_eu_hourly_anoms.nc'#'/home/jovyan/test_surge_models/input/CoDEC_ERA5_at_gesla3_tgs_eu_hourly_anoms.nc'\n",
    "out_path = '/home/jovyan/surgeNN/results/gtsm/CoDEC_ERA5_at_gesla3_tgs_eu_performance.nc'#'/home/jovyan/test_surge_models/results/gtsm/performance/CoDEC_ERA5_at_gesla3_tgs_eu_performance.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d67ef00-670a-403b-88ec-f5511ed7b773",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/xarray/backends/api.py:982: RuntimeWarning: Failed to open Zarr store with consolidated metadata, but successfully read with non-consolidated metadata. This is typically much slower for opening a dataset. To silence this warning, consider:\n",
      "1. Consolidating metadata in this existing store with zarr.consolidate_metadata().\n",
      "2. Explicitly setting consolidated=False, to avoid trying to read consolidate metadata, or\n",
      "3. Explicitly setting consolidated=True, to raise an error in this case instead of falling back to try reading non-consolidated metadata.\n",
      "  datasets = [open_(p, **open_kwargs) for p in paths]\n"
     ]
    }
   ],
   "source": [
    "lstms = io.Output('gs://leap-persistent/timh37/surgeNN_output/nns/performance_modified_v2/lstm') #load LSTM output to get the timestpe indices to evaluate\n",
    "lstms.open_performance_data(tgs)\n",
    "lstms.data = lstms.data.sel(max_timesteps_between_extremes=max_timesteps_between_extremes).load()\n",
    "\n",
    "observed_thresholds = lstms.observed_thresholds()\n",
    "observed_stds = lstms.observed_stds()\n",
    "\n",
    "lstms=lstms.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed35ec7a-d221-4005-90a5-ec26a95d8ab0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "codec = xr.open_dataset(gtsm_path) #anomalies wrt annual means\n",
    "codec['surge'] = deseasonalize_da(codec['surge']) #remove seasonal cycle (as done from the predictands)\n",
    "codec = codec.sel(tg=tgs) #select tide gauges\n",
    "codec = codec.sel(time=np.intersect1d(codec.time,lstms.time)) #select at 3-hourly timesteps of neural network predictions\n",
    "\n",
    "#compute error metrics:\n",
    "codec = codec.surge.expand_dims({'split':3},axis=-1).where(lstms.o.isel(it=0))\n",
    "codec = codec.to_dataset()\n",
    "\n",
    "where_observed_peaks = (lstms.o.isel(it=0)>=observed_thresholds)\n",
    "\n",
    "if max_timesteps_between_extremes>0:\n",
    "    where_observed_peaks = ((where_observed_peaks) & (where_observed_peaks.rolling(time=1+2*int(max_timesteps_between_extremes),center='True').sum()>1)) #from 'compute_statistics_on_output_ds'\n",
    "\n",
    "codec['rmse_bulk'] = np.sqrt(((lstms.o.isel(it=0) - codec.surge)**2).mean(dim='time'))\n",
    "codec['r_bulk'] = xr.corr(lstms.o.isel(it=0) ,codec.surge,dim='time')\n",
    "\n",
    "codec['rmse_extremes'] = np.sqrt(((lstms.o.isel(it=0).where(where_observed_peaks) - codec.surge.where(where_observed_peaks))**2).mean(dim='time'))\n",
    "codec['r_extremes'] = xr.corr(lstms.o.isel(it=0).where(where_observed_peaks),codec.surge.where(where_observed_peaks),dim='time')\n",
    "\n",
    "codec_exceedances = codec.surge>=observed_thresholds\n",
    "observed_exceedances = where_observed_peaks\n",
    "\n",
    "codec['true_pos'] =  ((where_observed_peaks) & (codec_exceedances)).where(np.isfinite(lstms.o.isel(it=0))).sum(dim='time')\n",
    "codec['false_neg'] =  ((where_observed_peaks) & ((codec_exceedances)==False)).where(np.isfinite(lstms.o.isel(it=0))).sum(dim='time')\n",
    "codec['false_pos'] =  (((where_observed_peaks)==False) & (codec_exceedances)).where(np.isfinite(lstms.o.isel(it=0))).sum(dim='time')\n",
    "codec['true_neg'] =  (((where_observed_peaks)==False) & ((codec_exceedances)==False)).where(np.isfinite(lstms.o.isel(it=0))).sum(dim='time')\n",
    "\n",
    "#confusion matrix derivatives\n",
    "codec['precision'] = compute_precision(codec.true_pos,codec.false_pos)\n",
    "codec['recall'] = compute_recall(codec.true_pos,codec.false_neg)\n",
    "codec['f1'] = compute_f1(codec.precision,codec.recall)\n",
    "\n",
    "for metric in ['r_extremes','rmse_extremes','true_pos','false_neg','false_pos','true_neg','precision','recall','f1']:\n",
    "    codec[metric] = codec[metric].expand_dims(dim='max_timesteps_between_extremes') #add additional dimension for max time distance used between \n",
    "codec['max_timesteps_between_extremes'] = [max_timesteps_between_extremes]\n",
    "\n",
    "if save_performance:\n",
    "    codec.to_netcdf(out_path,mode='w')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
